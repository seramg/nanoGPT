{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self attention for a single individual head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4 : self attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # 4 x 8 arrangement of tokens and each token has a information which is 32 dimensional embedded inside\n",
    "x = torch.randn(B, T,C ) # currently working with just random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In early cases, we will be doing a simple average and we dont actually want this to be all uniform because different tokens will find different other tokens more or less interesting and we want that to be data dependent so for example, if I am a vowel then maybe we must look for consonants in my past and maybe i want to know what that those consonants are and I want the information to flow to me but I want to now gather info from the past, but in a way its data dependent and this is the problem that the self attention solves.\n",
    "\n",
    "The way it solves is, every single node or every single token at each position will emit 2 vectors and it will emit a query and it will emit a key, now the query vector roughly speaking is what i am looking for and the key vector roughly speaking is what do i contain and then the way we get the affinities between these tokens now in a sequence is we basically just do a product between keys and queries so my query dot products with all the keys of all the other tokens and the dot product now becomes wei  and so, if key and query are sort of aligned they will interact to very high amount and then i will get to learn more about specific token as opposed to any other token in the sequence, so lets implement this now and we are going to implement a single whats called head of self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
      "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
      "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
      "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
      "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
      "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
      "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
      "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "# lets see a single head perform self attention\n",
    "head_size = 16  # hyper parameter\n",
    "key = nn.Linear(C, head_size, bias= False) # matrix multiple with some fixed weights\n",
    "query = nn.Linear(C, head_size, bias= False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# produce k and q by forwarding these modules on X, \n",
    "# all the tokens in all the positions in the B X T arrangement, all of them in parallel and independently produce a key and a query\n",
    "# no communication has happened yet but it comes now\n",
    "\n",
    "# all the queries will now get dot product with keys\n",
    "wei =  q @ k.transpose(-2, -1) # for it to be matrix multiplied, we need to keep the B as it is and then trnaspose the last 2 dimensions\n",
    "# (B, T,16) @ (B, 16, T) -------->  (B, T, T)\n",
    "print(wei[0])\n",
    "# for every row of B, we are now going to have a T square matrix giving us the affinities.\n",
    "# therefore these are not zero and they are resulted as a value after their communication \n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # all elements where tril is 0, we get -inf\n",
    "# version 3: use softmax \n",
    "wei = F.softmax(wei, dim = -1) # softmax over every single row, \n",
    "# softmax is also a normalisation operation and we get the exact same matrix\n",
    "out = wei @ x\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted aggregation now is a function in a data dependent manner between the keys and queries of these nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei\n",
    "# earlier wei was just constatnt, but now every single batch element will have different sort of wei\n",
    "# every single batch element contains different tokens at different positions and so this is not data dependent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391] knows what content it has and it knows at what position it is in and now the 8th token based on that creates a query : hey I am looking for this kind of stuff, I am a vowel and on the 8th position and I am looking for any consonants at positions upto 4 \n",
    "<br><br>\n",
    "and then all the nodes get to emit keys and maybe one of the channels could be I am a consonant and I am in a position upto 4 and that key would have a high number in that specific channel and thats how the query and the key when they dot product they can find each other and creates a high affinity and when they have high affinity like say this token was pretty interesting to this eight token when they have high affinity\n",
    "<br><br>\n",
    "and then through softmax, I will end up aggregating a lot of its info into my position and so I will get to learn a lot about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the masking and the softmax, wei comes out like the outputs of the dot products and these are raw outputs and they taken on values from -2 to +2 etc. so thats the raw interactions and raw affinities between all the nodes.\n",
    "<br><br>\n",
    "Y masking: Obviously if I am a fifth node, then I would not want to aggregate anything from 6th or 7th or 8th node, so we end up masking the upper triangular area and so those are not allowed to communicate\n",
    "<br><br>\n",
    "Y softmax : To get a nice distribution, to aggregating negative of this node, we exponentiate and normalise and we get a distribution that sums to 1 and this is telling us now in the data dependent manner how much of info to aggregate from any of these tokens in the past and not zeros as previous calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1915e+00, -1.1196e-01,  5.9079e-01,  3.6801e-01, -1.3106e+00,\n",
      "          4.3066e-01,  5.8015e-01, -2.1989e-01],\n",
      "        [-3.3711e-01,  3.4888e+00, -1.9695e+00,  3.3830e-01,  5.3988e+00,\n",
      "          7.9434e-03,  9.7835e-01,  1.0659e+00],\n",
      "        [ 2.4687e+00,  3.3016e+00,  3.9965e-01, -3.3672e-01,  3.2398e-01,\n",
      "          2.1113e+00, -2.6735e-01,  3.7084e-01],\n",
      "        [ 1.9890e+00, -1.6711e+00, -1.3341e+00, -1.0760e+00,  6.1292e-01,\n",
      "          1.2301e+00, -2.2057e+00, -8.6696e-01],\n",
      "        [-3.7363e-02,  9.3738e-01,  1.0745e+00,  1.3520e+00, -5.7113e-02,\n",
      "         -2.2037e+00,  2.5697e-01,  1.8758e+00],\n",
      "        [-7.9295e-01, -1.9818e+00,  1.1721e+00, -4.1350e-01,  1.6827e+00,\n",
      "         -8.6104e-02, -4.3308e-01,  1.1520e+00],\n",
      "        [-8.6515e-01, -3.7796e-01, -1.5812e+00,  1.9181e+00, -4.6045e-01,\n",
      "          2.0966e+00, -8.0967e-01, -2.6921e-01],\n",
      "        [ 3.7686e-02,  5.3952e-01,  4.7238e-01,  1.1007e+00,  3.5744e-01,\n",
      "         -1.0245e+00,  2.3461e-03,  1.6056e+00]], grad_fn=<SelectBackward0>)\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "head_size = 16  \n",
    "key = nn.Linear(C, head_size, bias= False) \n",
    "query = nn.Linear(C, head_size, bias= False)\n",
    "value = nn.Linear(C, head_size, bias= False)\n",
    "\n",
    "k = key(x) \n",
    "q = query(x) \n",
    "\n",
    "wei =  q @ k.transpose(-2, -1) \n",
    "\n",
    "print(wei[0])\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "\n",
    "wei = F.softmax(wei, dim = -1) \n",
    "\n",
    "v = value(x) \n",
    "out = wei @ v\n",
    "print(out.shape) # output will be 16 dim because that is the head sizeand so u can think of x as private info to this token\n",
    "# x is the info of the fifth token at som position and I have some identity and my info is kept in x and now for the purposes of single head, I am interested in is what I have and if you find me interesting and I will communicate to you and thats stored in v.\n",
    "# so v is the thing that gets aggregated for the purposes of this single head between the different nodes and self attention mechanism"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Attention is a communication mechanism where you have number of nodes in a directed graph where the edges will be pointed between nodes like this\n",
    "\n",
    "    ![image.png](attachment:image.png)\n",
    "\n",
    "and every node has some vector of information and it gets to aggregate information via a weighted sum from all of the nodes that points to it and this is done in a data dependent manner so depending on whatever data is actually stored at any point in time.\n",
    "\n",
    "<br>\n",
    "Our graph doesn't look like this, we have 8 nodes because block_size is 8 and then there is always 8 tokens and the first node is only pointed to itself  and second node will be pointed by the first node and to itself all the way upto 8th node which will pointed to by all the previous nodes and itself and so thats the structure that our directed graph has or happens to have in an auto regressive sort of scenario like language modelling but in principle attention can be applied to any arbitrary directed graph and its just a communication mechanism between nodes\n",
    "\n",
    "- There is no notion of space so attention simply acts over like a set of vectors in this graph and so by default, these nodes have no idea were they are positioned in the space and thats why we need to encode them positionally and sort of give them info that is anchored to a specific position so they sort of know where they are and this is different than for example from convolution because if you run a convolution operation over some input there is very specific sort of layout of the information in space and the convolution filters sort of act in space and so its not like an attention. In attention, its just a set of vectors out there in space they communicate and if you want them to have a notion of space, you need to specifically add it which is what we have done when we calculated the relative the positional encode encodings and added info to the vectors.\n",
    "\n",
    "- elements across the bacth dimension which are independent examples, never talk to each other. They are always processed independently and this is a batched matrix multiply that applies basically a matrix multiplication kind of in parallel across the batch dimension \n",
    "\n",
    "    wei =  q @ k.transpose(-2, -1) \n",
    "\n",
    "    so maybe it would be more acccurate to say that in this analogy of the directed graph because batch size is 4 we really have 4 separate pools of 8 nodes and those 8 nodes only talk to each other but in total there is like 32 nodes that are being processed but there is sort of 4 separated pools of 8 \n",
    "\n",
    "- We have a specific structure of directed graph where the future tokens will not communicate to the past tokens but this doesn't necessarily have to be the constraint in the general case and in fact in many cases, you want to have all of the nodes talk to each other fully, so as an example, if you are doing sentimental analysis or something like that with a transformer you might have a number of tokens and you may want to have them all talk  to each other fully because later you are predicting for a sentiment of the sentence so its okay for these nodes to talk to each other and so in these cases, you will use an encoder block of self attention adn all it means that its an encoder block is that you will delete this line of code $ wei = wei.masked_fill(tril == 0, float('-inf')) $  allowing all the nodes to completely talk to each other what we are implementing here is sometimes called decoder block and its called decoder because its sort of like decoding language and its got this auto regressive format where you have to mask with the triangular matrix so that nodes from the future never talk to the past because they would give away the answer and so basically in encoder blocks you would delete this $ wei = wei.masked_fill(tril == 0, float('-inf')) $ allow all the nodes to talk and in decoder blocks this $ wei = wei.masked_fill(tril == 0, float('-inf')) $ will always be present so that u have this triangular structure but both are allowed and attention doesn't care, attention supports arbitrary connectivity between nodes.\n",
    "\n",
    "- Reason the attention we found is self attention is because the keys and queries and the values are all coming from the same source (from x) so the same source x produce keys queries and values so these nodes are self attending but in principle attention is much more general than that.\n",
    "\n",
    "    For example, an encoder decoder transformers can have a case where the queries are produced from x but keys and values come from a whole separate external source and sometimes from encoder blocks that encode some context that we would like to condition on and so the keys and values will actually come from a whole separate source those are nodes on the side and here we are just producing queries and we are reading off info from the side so cross attention is used when there is separate source of nodes we would like to pull info from into our nodes and its self attention if we just have nodes that would like to look at each other and talk to each other so this attention here happens to be self attention but in principle, attention is lot more general.\n",
    "\n",
    "- When we look into the paper, we see \n",
    "\n",
    "    ![image-2.png](attachment:image-2.png)\n",
    "\n",
    "    where $ d_k $ is the head size.\n",
    "\n",
    "    We have done everything but dividing by square root of head size is not done.\n",
    "    They call this the scaled attention and its kind of like an important normalisation to basically have k nd q\n",
    "\n",
    "    key = torch.randn(B, T, head_size) <br>\n",
    "    query = torch.randn(B, T, head_size)\n",
    "\n",
    "    the zero mean unit variance k and q are unit gaussian and then if we calculate wei naively, it will be the variance which will be on the order of head_size which in our case is 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.6696)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "k.var()\n",
    "q.var()\n",
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1445)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q= torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # multiply by 1/ square root of head size\n",
    "# k.var()\n",
    "# q.var() \n",
    "wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be one as it will be preserved. This is very important because wei will be fed into softmax and its really important especially at initialization that we be fairly diffuse, so in our case, we locked out here and $ wei $ had a fairly diffuse members so the problem is that because of softmax if weight takes on very positive and very negative numbers inside it softmax will actually converge towards one hot vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are applying softmax to a tensor of values that are very close to zero, then we are diffusing out of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the moment I take the exact same thing and I start sharpening it making it bigger by multiplying these numbers by 8 for example, we will see that the softmax will start to sharpen and in fact it will sharpen towards the max, to whatever number here that is the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we dont want these values to be too extreme especially at initialisation otherwise softmax will be way too peaky and we are basically aggregating information from like single node, every node just aggregates info from a single other node and thats not what we want especially at initialisation and so scaling is used just to control the variance at initialisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
