{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters 1115394\n"
     ]
    }
   ],
   "source": [
    "print('length of dataset in characters', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text))) # list causes an arbitrary ordering\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here, we have a total of 65 characters in total that are possible elements of our sequences that the model can see or emit including a space at the beginning followed by special characters, caps and small alphabets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.     TOKENIZATION\n",
    "\n",
    "First step is to tokenize which means the convert the raw text as a string to some sequence of integers according to some notebook, a vocabulary of possible elements.\n",
    "\n",
    "In our case, its simply going to be translating individual characters into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    " # Encoder and decoder\n",
    " \n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encode a string input and output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decode a list of integers input and output a string\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[46, 47, 1, 58, 46, 43, 56, 43]\n",
    "represents string and when decoded, due to a reverse mapping between the value and encoded value, returns the exact same text.\n",
    "\n",
    "Therefore this could be seen as a translation to integers and back for an arbitrary string done on a character level.\n",
    "\n",
    "What we did is :\n",
    "\n",
    "- Iterate over all the characters\n",
    "- Create a lookup table from the character to the integer and vice versa\n",
    "- Encode some string by translating all characters individually to list of integers\n",
    "- Decode it back, we use reverse mapping and concatenate all of them.\n",
    "\n",
    "\n",
    "Other type of schemas exist.\n",
    "\n",
    "- Google use **[SentencePiece](https://github.com/google/sentencepiece)** which encode text into integers but in different schema and vocabulary. **SentencePiece** is a subword, a sort of tokenizer which wont be encoding entire words or individual characters but a subword unit level\n",
    "\n",
    "- OpenAI uses **[tiktoken](https://github.com/openai/tiktoken)** library taht use a byte pair encoding tokenizer. (used for GPT too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.n_vocab # instead of 65, we have 50k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 4178, 612]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"hii there\")\n",
    "\n",
    "# Unlike the previous case, we get only 3 integers where they are not between 0 and 64 but between Z and 50,257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "``` encode(\"hi there\")  [46, 47, 1, 58, 46, 43, 56, 43]```\n",
    " \n",
    " ```enc.encode(\"hii there\") [71, 4178, 612]```\n",
    "\n",
    " A trade off happens between the code book size and the sequence lengths enabling us to have very long sequences of integers with very small vocabularies (character level tokenizer) or we could have short sequences of integers with very large vocabularies (subword). \n",
    "\n",
    " Typically people use subword encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Applying a character level tokenizer for the entire training set of Shakespeare\n",
    "\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # 1000 characters will be shown as how it would look for the GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequence of integers is the identical translation of the first 1000 characters where each of the number is >= 0  and <=65 where zero cud be a newline character and 1 a space and it goes on.\n",
    "\n",
    "Basically a direct translation to integers happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up into train and validation sets for the transformer.\n",
    "n = int(0.9*len(data)) # 90% is train and rest 10% val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:] # help us understand to what extend, our model is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are never going to feed the text into Transformer all at once cause it would be computationally very expensive and prohibitive.\n",
    "\n",
    "So it will be just chunks of data set that the Transformer will be working on and when we train it, we give sample random little chunks of the training data and will train on just chunks at a time. These chunks have a particular length and that maximum length is called block size/ context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While sampling a chunk of data like this, it has multiple examples packed into it because all of these characters follow each other and so plugging on these to the Transformer is that we are going to simultaneously train it to make predictions at every one of the positions.\n",
    "\n",
    "In a chunk of 9 characters, there is actually 8 individual characters packed in there. \n",
    "\n",
    "In context of 18---> 47 comes up <br>\n",
    "In context of 18,47 ---> 56 comes up <br>\n",
    "In context of 18,47,56 ---> 57 comes up <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "# TIME DIMENSION\n",
    "x = train_data[:block_size] # inputs of the transformer, first block size characters \n",
    "y = train_data[1:block_size + 1] # next block size characters offset by 1. y is the targets for each position in the input\n",
    "for t in range(block_size): # going over 8\n",
    "    context = x[:t+1] # context is always the 9 characters (upto t incl t)\n",
    "    target = y[t] # target\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 examples with context between 1 to context of block size hidden in the chunk of 9 characters that we sampled from the dataset. We train for not just computational reasons because we happen to have sequence already, its not done for efficiency but also to make the Transformer network be used to seeing contexts all the way from as little as one to block size. This way, transformer wud be able to see everything in between and thats useful during inference.\n",
    "\n",
    "During sampling, we cud start the sampling generation as little as 1 character and the Transformer wud be able to predict the next character with all the way up to just context of 1 and then can predict everything up to block size and after block size, we start truncating because Transformer when predicting the next character, wont receive more than a block size input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we sample the chunks of text, every time we feed into the Transformer, we will have many batches of multiple chunks of texts that are all stacked up in a single tensor. This is for efficiency and also keep the gpus busy because they r very good at parallel processing of data.\n",
    "\n",
    "In this way, we want to process multiple chunks all at the same time but those chunks are processed completely independently they don't talk to each other and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "======\n",
      "When input is [24] the target: 43\n",
      "When input is [44] the target: 53\n",
      "When input is [52] the target: 58\n",
      "When input is [25] the target: 17\n",
      "When input is [24, 43] the target: 58\n",
      "When input is [44, 53] the target: 56\n",
      "When input is [52, 58] the target: 1\n",
      "When input is [25, 17] the target: 27\n",
      "When input is [24, 43, 58] the target: 5\n",
      "When input is [44, 53, 56] the target: 1\n",
      "When input is [52, 58, 1] the target: 58\n",
      "When input is [25, 17, 27] the target: 10\n",
      "When input is [24, 43, 58, 5] the target: 57\n",
      "When input is [44, 53, 56, 1] the target: 58\n",
      "When input is [52, 58, 1, 58] the target: 46\n",
      "When input is [25, 17, 27, 10] the target: 0\n",
      "When input is [24, 43, 58, 5, 57] the target: 1\n",
      "When input is [44, 53, 56, 1, 58] the target: 46\n",
      "When input is [52, 58, 1, 58, 46] the target: 39\n",
      "When input is [25, 17, 27, 10, 0] the target: 21\n",
      "When input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "When input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# Start sampling random locations in the data set to pull chunks from. For that we need to set a seed\n",
    "torch.manual_seed(1337) # setting up the seed\n",
    "batch_size = 4  # how many independent sequences will we process in parallel or every forward backward pass of the transformer?\n",
    "block_size = 8  # the maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate random positions to grab a chunk out of. Generate the batch size number of random offsets\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # first block size characters starting at i \n",
    "    # stack 1 d tensors into row in a 4x8 tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])# y's are offset by 1s\n",
    "    return x, y # get the chunks for every i and ix \n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb) # each row is a chunk of a training set.\n",
    "# this 4x8 array contains 32 examples and completely independent of the transformer\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb) # to create the loss fn\n",
    "\n",
    "print('======')\n",
    "\n",
    "for t in range(block_size):  # batch dimension\n",
    "    for b in range(batch_size):  # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b][t]\n",
    "        print(f\"When input is {context.tolist()} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input is 24, target is 43<br>\n",
    "Input 24,43 target 58<br>\n",
    "all 32 examples packed into a single batch of input x nd desired targets in y<br>\n",
    "\n",
    "X is going to feed into transformer and it simultaneously process all the examples and then lookup the correct integers to predict in every one of these positions in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEED THIS INTO THE NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting of with the most simplest possible neural network in case of language modelling - BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageMOdel(nn.Module): # sub class of nn module\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # thin tensor of shape vocab_size by vocab_size\n",
    "        \n",
    "    def forward(self, idx, targets): # index= idx \n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # passing the index \n",
    "        return logits\n",
    "    \n",
    "m = BigramLanguageMOdel(vocab_size)\n",
    "out = m(xb,yb) # inp xb and targets yb\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every single integer in our input is going to refer to this embedding table and pluck out a row of the embedding table corresponding to the index\n",
    "\n",
    "Say 24 in the input, will go to the embedding table and pluck out the 24th row.\n",
    "pytorch will then arrange all of this into a (B= Batch, T= Time, C= channel) tensor\n",
    "B = 4, T = 8 and C = vocab_size = 65\n",
    "\n",
    "This step will pluck out rows from the table and arrange it like this and this is interpreted as logits which are nothing but scores for the next character in the sequence\n",
    "\n",
    "We are actually predicting what comes next based on the individual identity of the single token.\n",
    "\n",
    "Currently, the tokens are not talking to each other and they are not seeing any context except for they see themselves. Basically a token number 5 says it can make decent predictions about what comes next just by knowing it is a token number 5 because some character follow some certain other character in typical scenarios.\n",
    "\n",
    "torch.Size([4, 8, 65]), we get the scores or predictions for every single 4x8 positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING THE LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to measure a loss or like a quality of predictions is to use the neg log likelihood loss which is implemented as cross_entropy in pytorch. loss is the quality of logits with respect to targets. Given the Identity of the next character, how well we could predict the next character based on the logits.\n",
    "\n",
    "The correct dimension of logits depending on whatever target is, should have a very high number and all other dimensions should be very low number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 65], got [4, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logits,loss\n\u001b[1;32m     21\u001b[0m m \u001b[38;5;241m=\u001b[39m BigramLanguageMOdel(vocab_size)\n\u001b[0;32m---> 22\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# inp xb and targets yb\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m, in \u001b[0;36mBigramLanguageMOdel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, targets): \u001b[38;5;66;03m# index= idx \u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#idx and targets are both (B,T) tensor of integers\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(idx) \u001b[38;5;66;03m# passing the index \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# cross entropy on predictions and targets\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits,loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 65], got [4, 8]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageMOdel(nn.Module): # sub class of nn module\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # thin tensor of shape vocab_size by vocab_size\n",
    "        \n",
    "    def forward(self, idx, targets): # index= idx \n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # passing the index \n",
    "        loss = F.cross_entropy(logits, targets) # cross entropy on predictions and targets\n",
    "        \n",
    "        return logits,loss\n",
    "    \n",
    "m = BigramLanguageMOdel(vocab_size)\n",
    "logits, loss = m(xb,yb) # inp xb and targets yb\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a error here because pytorch library has different configurations for inputs.\n",
    "\n",
    "For a multi dimensional arrays, its set as B by C by T instead of a B by T by C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageMOdel(nn.Module): # sub class of nn module\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # thin tensor of shape vocab_size by vocab_size\n",
    "        \n",
    "    def forward(self, idx, targets): # index= idx \n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # passing the index \n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C) # B and T are crushed to a  1d and C is the 2nd dimension. It is stretched out from a 3 to a 2 dimensional.\n",
    "        targets = targets.view(-1)\n",
    "        # targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets) # cross entropy on predictions and targets\n",
    "        \n",
    "        return logits,loss\n",
    "    \n",
    "m = BigramLanguageMOdel(vocab_size)\n",
    "logits, loss = m(xb,yb) # inp xb and targets yb\n",
    "print(logits.shape)\n",
    "print(loss) # because we have 65 possible vocabulary elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are expecting a -ln(1/65) which is 4.17 and we get a 4.87.\n",
    "\n",
    "From this we understand, the initial predictions are not super diffuse and yes they got a bit of entropy so we were guessing wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE FROM THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageMOdel(nn.Module): # sub class of nn module\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # thin tensor of shape vocab_size by vocab_size\n",
    "        \n",
    "    def forward(self, idx, targets = None): # index= idx \n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # passing the index \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # B and T are crushed to a  1d and C is the 2nd dimension. It is stretched out from a 3 to a 2 dimensional.\n",
    "            targets = targets.view(-1)\n",
    "            # targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # cross entropy on predictions and targets\n",
    "        \n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): # idx is (B, T) array of indices in the current context in some batch\n",
    "        # task of the generate function is to take the B x T and extend it to by B x T+1, B x T+2, therefore it continues generation in all the batch dimensions in the time dimensions and do for max new tokens \n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits,loss = self(idx) # loss is ignored. no targets to compare\n",
    "            logits = logits[: , -1, :] # from the BxTxC, we pluck out the -1,last dimension in the time dimension because those are predictions for what comes next.\n",
    "            # softmax of logits give us probabilities\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            # sample from the probabilities and we get 1 sample.\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1), from each one of the batch dimension we will have a single prediction for what comes next\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # integer idx_next get concatenated on top of the current stream of integers and give us BxT+1\n",
    "            # whatever is predicted is concated on top of the previous idx along the first dimension (dim=1) which is the time dimension to create BxT+1, so that because a new idx.\n",
    "        return idx\n",
    "                    \n",
    "m = BigramLanguageMOdel(vocab_size)\n",
    "logits, loss = m(xb,yb) # inp xb and targets yb\n",
    "print(logits.shape)\n",
    "print(loss) # because we have 65 possible vocabulary elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype= torch.long) # a tensor 1 x 1 with batch 1 and time 1 and holds all 0s\n",
    "# zero kicks of the generations and it is actually a new line character\n",
    "\n",
    "generation = decode(m.generate(idx, max_new_tokens=100)[0].tolist()) # 100 tokens\n",
    "# Generate will continue this.\n",
    "# since it works on level of batches, then we will have to index into the 0th row and basically unplug the single batch dimension that exists and \n",
    "# then that gives us time steps that is a 1 d array of all the indices which is converted to a python list from pytorch tensor .\n",
    "# This can be fed into our decode fn and convert those integers into text.\n",
    "\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is totally a garbage. it is a totally random model.\n",
    "This function needs to general but its not right as in we are feeding all this without building out the context and we concatenate it all and we feeding it all of them.\n",
    "\n",
    "Actually even if we are feeding in the entire sequence, but they look only at the last piece and predict the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE BIGRAM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training help us make this less random. Instead of a stochastic gradient descent which is the most simplest possible optimizer \n",
    "\n",
    "While AdamW is the more advanced and popular optimizer and works extremely well for a learning rate roughly 3e-4 but for very very small networks like this, we can have learning rates of 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer works by taking the gradients computed during backpropagation and applying these gradients to update the model's parameters. In other words, it adjusts the parameters (weights, biases, embeddings, etc.) in the direction that reduces the error (loss) in the model’s predictions.\n",
    "\n",
    "It’s based on the concept of stochastic gradient descent (SGD) but with improvements like adaptive learning rates and momentum, making training faster and more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create a pytorch optimization object\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\n",
    "# A variant of the Adam optimizer that includes weight decay (a regularization technique) to help prevent overfitting.\n",
    "# takes the gradients and update the parameters using the gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4571714401245117\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # zeros out all the gradients from the previous step and getting gradients for all params\n",
    "    # using those gradients, we update our params\n",
    "    loss.backward()\n",
    "    optimizer.step() # update\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trerrs, nes hthemuretirs, fonoumew carerted, w l' fryeathenill ws y thon astidispl m.\n",
      "We linerny.\n",
      "MAur hale, whour, bud shel--avesen, p\n",
      "S:\n",
      "TUS:\n",
      "S:\n",
      "\n",
      "\n",
      "Towingorshivous icarout at I ncars Praves sorsee itesis arifr m geon h othayer,\n",
      "\n",
      "And te tou bldo alalide m\n",
      "\n",
      "The wf ld sall finthe! h hom sbut f rayoooest ENGO:\n",
      "TAUShe hen mim, ler ety tst,\n",
      "I s\n",
      "BYowisttoue flle or s, bous, w\n",
      "\n",
      "\n",
      "arrindfre.\n",
      "Hoond w f y prin aid.\n",
      "\n",
      "BEToom me, ongahe GBimad ry\n",
      "Wherd\n",
      "\n",
      "SThour me t INNGO my n d fFose:\n",
      "TELENAu, morchorve tond \n"
     ]
    }
   ],
   "source": [
    "generation = decode(m.generate(idx = torch.zeros((1,1), dtype= torch.long), max_new_tokens=500)[0].tolist()) \n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some kind of improvement happened here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
